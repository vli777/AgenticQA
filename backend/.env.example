# Environment Variables for Railway Deployment
# Copy this file to .env and fill in your actual values

# Debug mode (set to "false" for production)
DEBUG=false

# API Keys
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
NVIDIA_API_KEY=your_nvidia_api_key_here

# Pinecone Configuration
PINECONE_INDEX_NAME=test

# CORS Origins (comma-separated list for production)
CORS_ORIGINS=https://your-frontend-domain.com,https://another-domain.com

# Embedding Model (choose one of: multilingual-e5-large, text-embedding-3-small, llama-text-embed-v2)
EMBEDDING_MODEL=text-embedding-3-small

# Hybrid Search Configuration (Best Practice Pipeline)
# Number of BM25 candidates (lexical search, recommended: 30)
BM25_K=30

# Number of vector candidates (semantic search, recommended: 30)
VECTOR_K=30

# CROSS_ENCODER_MODEL: Model to use for re-ranking (HuggingFace model name)
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Performance Optimization Features
# Enable caching for embeddings, LLM responses, and search results
ENABLE_CACHING=true

# Enable token streaming for LLM responses
ENABLE_STREAMING=true

# Cache Configuration
# Embedding cache settings (size: number of embeddings, TTL: seconds)
EMBEDDING_CACHE_SIZE=10000
EMBEDDING_CACHE_TTL=7200

# Search results cache settings
SEARCH_CACHE_SIZE=1000
SEARCH_CACHE_TTL=1800

# LLM response cache settings (only caches deterministic responses, temperature=0)
LLM_CACHE_SIZE=500
LLM_CACHE_TTL=3600